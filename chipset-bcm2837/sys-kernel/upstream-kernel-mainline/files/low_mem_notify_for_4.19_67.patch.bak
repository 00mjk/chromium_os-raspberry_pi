diff --git a/drivers/char/mem.c b/drivers/char/mem.c
index 7b4e4de778e4..70b23103d2c6 100644
--- a/drivers/char/mem.c
+++ b/drivers/char/mem.c
@@ -29,6 +29,7 @@
 #include <linux/export.h>
 #include <linux/io.h>
 #include <linux/uio.h>
+#include <linux/low-mem-notify.h>
 
 #include <linux/uaccess.h>
 
@@ -877,6 +878,9 @@ static const struct memdev {
 #ifdef CONFIG_PRINTK
 	[11] = { "kmsg", 0644, &kmsg_fops, 0 },
 #endif
+#ifdef CONFIG_LOW_MEM_NOTIFY
+  [12] = { "chromeos-low-mem", 0666, &low_mem_notify_fops, 0 },
+#endif
 };
 
 static int memory_open(struct inode *inode, struct file *filp)
diff --git a/include/linux/low-mem-notify.h b/include/linux/low-mem-notify.h
new file mode 100644
index 000000000000..927eda5a4ecb
--- /dev/null
+++ b/include/linux/low-mem-notify.h
@@ -0,0 +1,138 @@
+#ifndef _LINUX_LOW_MEM_NOTIFY_H
+#define _LINUX_LOW_MEM_NOTIFY_H
+
+#include <linux/mm.h>
+#include <linux/ratelimit.h>
+#include <linux/stddef.h>
+#include <linux/swap.h>
+
+/* We support up to this many different thresholds. */
+#define LOW_MEM_THRESHOLD_MAX 5
+
+extern unsigned long low_mem_thresholds[];
+extern unsigned int low_mem_threshold_count;
+extern unsigned int low_mem_threshold_last;
+void low_mem_notify(void);
+extern const struct file_operations low_mem_notify_fops;
+extern bool low_mem_margin_enabled;
+extern unsigned long low_mem_lowest_seen_anon_mem;
+extern unsigned int low_mem_ram_vs_swap_weight;
+extern struct ratelimit_state low_mem_logging_ratelimit;
+
+#ifdef CONFIG_SYSFS
+extern void low_mem_threshold_notify(void);
+#else
+static inline void low_mem_threshold_notify(void)
+{
+}
+#endif
+
+/*
+ * Compute available memory used by files that can be reclaimed quickly.
+ */
+static inline unsigned long get_available_file_mem(void)
+{
+	unsigned long file_mem =
+			global_node_page_state(NR_ACTIVE_FILE) +
+			global_node_page_state(NR_INACTIVE_FILE);
+	unsigned long dirty_mem = global_node_page_state(NR_FILE_DIRTY);
+	unsigned long min_file_mem = min_filelist_kbytes >> (PAGE_SHIFT - 10);
+	unsigned long clean_file_mem = file_mem - dirty_mem;
+	/* Conservatively estimate the amount of available_file_mem */
+	unsigned long available_file_mem = (clean_file_mem > min_file_mem) ?
+			(clean_file_mem - min_file_mem) : 0;
+	return available_file_mem;
+}
+
+/*
+ * Available anonymous memory.
+ */
+static inline unsigned long get_available_anon_mem(void)
+{
+	return global_node_page_state(NR_ACTIVE_ANON) +
+		global_node_page_state(NR_INACTIVE_ANON);
+}
+
+/*
+ * Compute "available" memory, that is either free memory or memory that can be
+ * reclaimed quickly, adjusted for the presence of swap.
+ */
+static inline unsigned long get_available_mem_adj(void)
+{
+	/* free_mem is completely unallocated; clean file-backed memory
+	 * (file_mem - dirty_mem) is easy to reclaim, except for the last
+	 * min_filelist_kbytes. totalreserve_pages is the reserve of pages that
+	 * are not available to user space.
+	 */
+	unsigned long raw_free_mem = global_zone_page_state(NR_FREE_PAGES);
+	unsigned long free_mem = (raw_free_mem > totalreserve_pages) ?
+			raw_free_mem - totalreserve_pages : 0;
+	unsigned long available_mem = free_mem +
+			get_available_file_mem();
+	unsigned long swappable_pages = min_t(unsigned long,
+			get_nr_swap_pages(), get_available_anon_mem());
+	/*
+	 * The contribution of swap is reduced by a factor of
+	 * low_mem_ram_vs_swap_weight.
+	 */
+	return available_mem + swappable_pages / low_mem_ram_vs_swap_weight;
+}
+
+#ifdef CONFIG_LOW_MEM_NOTIFY
+/*
+ * Returns TRUE if we are in a low memory state.
+ */
+static inline bool low_mem_check(void)
+{
+	static bool was_low_mem;	/* = false, as per style guide */
+	/* We declare a low-memory condition when a combination of RAM and swap
+	 * space is low.
+	 */
+	unsigned long available_mem = get_available_mem_adj();
+	/*
+	 * For backwards compatibility with the older margin interface, we will
+	 * trigger the /dev/chromeos-low_mem device when we are below the
+	 * lowest threshold
+	 */
+	bool is_low_mem = available_mem < low_mem_thresholds[0];
+	unsigned int threshold_lowest = UINT_MAX;
+	int i;
+
+	if (!low_mem_margin_enabled)
+		return false;
+
+	if (unlikely(is_low_mem && !was_low_mem) &&
+	    __ratelimit(&low_mem_logging_ratelimit)) {
+		pr_info("entering low_mem (avail RAM = %lu kB, avail swap %lu kB, avail file %lu kB, anon mem: %lu kB)\n",
+			available_mem * PAGE_SIZE / 1024,
+			get_nr_swap_pages() * PAGE_SIZE / 1024,
+			get_available_file_mem() * PAGE_SIZE / 1024,
+			get_available_anon_mem() * PAGE_SIZE / 1024);
+	}
+	was_low_mem = is_low_mem;
+
+	if (is_low_mem)
+		low_mem_notify();
+
+	for (i = 0; i < low_mem_threshold_count; i++)
+		if (available_mem < low_mem_thresholds[i]) {
+			threshold_lowest = i;
+			break;
+		}
+
+	/* we crossed one or more thresholds */
+	if (unlikely(threshold_lowest < low_mem_threshold_last))
+		low_mem_threshold_notify();
+
+	low_mem_threshold_last = threshold_lowest;
+
+	return is_low_mem;
+}
+#else
+static inline bool low_mem_check(void)
+{
+	return false;
+}
+#endif
+
+#endif
diff --git a/include/linux/mm.h b/include/linux/mm.h
index bdec425c8e14..ac0cda849e38 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -58,6 +58,8 @@ extern int sysctl_legacy_va_layout;
 #define sysctl_legacy_va_layout 0
 #endif
 
+extern int min_filelist_kbytes;
+
 #ifdef CONFIG_HAVE_ARCH_MMAP_RND_BITS
 extern const int mmap_rnd_bits_min;
 extern const int mmap_rnd_bits_max;
@@ -125,6 +127,7 @@ extern int mmap_rnd_compat_bits __read_mostly;
 #define DEFAULT_MAX_MAP_COUNT	(USHRT_MAX - MAPCOUNT_ELF_CORE_MARGIN)
 
 extern int sysctl_max_map_count;
+extern int sysctl_mmap_noexec_taint;
 
 extern unsigned long sysctl_user_reserve_kbytes;
 extern unsigned long sysctl_admin_reserve_kbytes;
@@ -2816,6 +2819,9 @@ void __init setup_nr_node_ids(void);
 #else
 static inline void setup_nr_node_ids(void) {}
 #endif
+#ifdef CONFIG_DISK_BASED_SWAP
+extern int sysctl_disk_based_swap;
+#endif
 
 #endif /* __KERNEL__ */
 #endif /* _LINUX_MM_H */
diff --git a/include/linux/mm_metrics.h b/include/linux/mm_metrics.h
new file mode 100644
index 000000000000..c699517aa40e
--- /dev/null
+++ b/include/linux/mm_metrics.h
@@ -0,0 +1,84 @@
+/* SPDX-License-Identifier: GPL-2.0+ */
+
+#ifndef _LINUX_MM_METRICS_H
+#define _LINUX_MM_METRICS_H
+
+#include <linux/timekeeping.h>
+#include <linux/sched/clock.h>
+#include <linux/swapops.h>
+
+struct histogram {
+	struct rcu_head rcu;
+	unsigned int size;
+	u64 __percpu *buckets;
+	u64 thresholds[0];
+};
+
+enum {
+	MM_SWAP_REFAULT,
+	MM_SWAP_LATENCY,
+	MM_RECLAIM_LATENCY,
+	NR_MM_METRICS,
+};
+
+extern struct histogram __rcu *mm_metrics_files[NR_MM_METRICS];
+
+#ifdef CONFIG_MM_METRICS
+#define mm_metrics_enabled(type)	rcu_access_pointer(mm_metrics_files[type])
+#else
+#define mm_metrics_enabled(type)	0
+#endif
+
+extern void mm_metrics_record(unsigned int type, u64 val, u64 count);
+
+static inline void mm_metrics_swapout(swp_entry_t *swap)
+{
+	if (mm_metrics_enabled(MM_SWAP_REFAULT)) {
+		u64 start = ktime_get_seconds();
+
+		VM_BUG_ON(swp_type(*swap) >= MAX_SWAPFILES);
+		VM_BUG_ON(!swp_offset(*swap));
+
+		swap->val &= ~GENMASK_ULL(SWP_TM_OFF_BITS - 1, SWP_OFFSET_BITS);
+		if (start < BIT_ULL(SWP_TIME_BITS))
+			swap->val |= start << SWP_OFFSET_BITS;
+	}
+}
+
+static inline void mm_metrics_swapin(swp_entry_t swap)
+{
+	if (mm_metrics_enabled(MM_SWAP_REFAULT)) {
+		u64 start = _swp_offset(swap) >> SWP_OFFSET_BITS;
+
+		VM_BUG_ON(swp_type(swap) >= MAX_SWAPFILES);
+		VM_BUG_ON(!swp_offset(swap));
+
+		if (start)
+			mm_metrics_record(MM_SWAP_REFAULT,
+					  ktime_get_seconds() - start, 1);
+	}
+}
+
+static inline u64 mm_metrics_swapin_start(void)
+{
+	return mm_metrics_enabled(MM_SWAP_LATENCY) ? sched_clock() : 0;
+}
+
+static inline void mm_metrics_swapin_end(u64 start)
+{
+	if (mm_metrics_enabled(MM_SWAP_LATENCY) && start)
+		mm_metrics_record(MM_SWAP_LATENCY, sched_clock() - start, 1);
+}
+
+static inline u64 mm_metrics_reclaim_start(void)
+{
+	return mm_metrics_enabled(MM_RECLAIM_LATENCY) ? sched_clock() : 0;
+}
+
+static inline void mm_metrics_reclaim_end(u64 start)
+{
+	if (mm_metrics_enabled(MM_RECLAIM_LATENCY) && start)
+		mm_metrics_record(MM_RECLAIM_LATENCY, sched_clock() - start, 1);
+}
+
+#endif /* _LINUX_MM_METRICS_H */
diff --git a/include/linux/sched/sysctl.h b/include/linux/sched/sysctl.h
index a9c32daeb9d8..77e9fa3c5b49 100644
--- a/include/linux/sched/sysctl.h
+++ b/include/linux/sched/sysctl.h
@@ -22,6 +22,8 @@ enum { sysctl_hung_task_timeout_secs = 0 };
 
 extern unsigned int sysctl_sched_latency;
 extern unsigned int sysctl_sched_min_granularity;
+extern unsigned int sysctl_sched_sync_hint_enable;
+extern unsigned int sysctl_sched_cstate_aware;
 extern unsigned int sysctl_sched_wakeup_granularity;
 extern unsigned int sysctl_sched_child_runs_first;
 
diff --git a/include/linux/swapops.h b/include/linux/swapops.h
index 22af9d8a84ae..170b5bbb6de7 100644
--- a/include/linux/swapops.h
+++ b/include/linux/swapops.h
@@ -47,7 +47,7 @@ static inline unsigned swp_type(swp_entry_t entry)
  * Extract the `offset' field from a swp_entry_t.  The swp_entry_t is in
  * arch-independent format
  */
-static inline pgoff_t swp_offset(swp_entry_t entry)
+static inline pgoff_t _swp_offset(swp_entry_t entry)
 {
 	return entry.val & SWP_OFFSET_MASK(entry);
 }
@@ -82,7 +82,7 @@ static inline pte_t swp_entry_to_pte(swp_entry_t entry)
 {
 	swp_entry_t arch_entry;
 
-	arch_entry = __swp_entry(swp_type(entry), swp_offset(entry));
+	arch_entry = __swp_entry(swp_type(entry), _swp_offset(entry));
 	return __swp_entry_to_pte(arch_entry);
 }
 
@@ -102,6 +102,64 @@ static inline void *swp_to_radix_entry(swp_entry_t entry)
 	return (void *)(value | RADIX_TREE_EXCEPTIONAL_ENTRY);
 }
 
+/*
+ * We squeeze swapout timestamp into swp_offset because we don't
+ * want to allocate extra memory for it. Normally we have 50 bits
+ * in swp_offset on x86_64 and arm64. So we use 25 bits for the
+ * timestamp and the rest for offset. The timestamp is uptime in
+ * second, and it won't overflow within one year. The max size of
+ * swapfile is 128G, which is more than enough for now. If we have
+ * less than 50 bits in swp_offset due to 32-bit swp_entry_t or
+ * X86_BUG_L1TF, we don't enable the timestamp.
+ */
+#define SWP_TIME_BITS  25
+#define SWP_OFFSET_BITS        25
+#define SWP_TM_OFF_BITS        (SWP_TIME_BITS + SWP_OFFSET_BITS)
+
+extern bool swap_refault_enabled __read_mostly;
+
+#ifdef CONFIG_MM_METRICS
+
+static inline pgoff_t swp_offset(swp_entry_t swap)
+{
+       return swap_refault_enabled && swp_type(swap) < MAX_SWAPFILES ?
+              _swp_offset(swap) & GENMASK_ULL(SWP_OFFSET_BITS - 1, 0) :
+              _swp_offset(swap);
+}
+
+static inline bool swp_entry_same(swp_entry_t s1, swp_entry_t s2)
+{
+       return swp_type(s1) == swp_type(s2) && swp_offset(s1) == swp_offset(s2);
+}
+
+static inline bool swp_page_same(swp_entry_t swap, struct page *page)
+{
+       swp_entry_t entry = { .val = page_private(page) };
+
+       VM_BUG_ON(!PageSwapCache(page));
+
+       return swp_entry_same(swap, entry);
+}
+
+static inline bool swp_radix_same(swp_entry_t swap, void *radix)
+{
+       return radix_tree_exceptional_entry(radix) &&
+              swp_entry_same(swap, radix_to_swp_entry(radix));
+}
+
+#else /* CONFIG_MM_METRICS */
+
+#define swp_offset(swap)               _swp_offset(swap)
+
+#define swp_entry_same(s1, s2)         ((s1).val == (s2).val)
+
+#define swp_page_same(swap, page)      ((swap).val == page_private(page))
+
+#define swp_radix_same(swap, radix)    (swp_to_radix_entry(swap) == (radix))
+
+#endif /* CONFIG_MM_METRICS */
+
+
 #if IS_ENABLED(CONFIG_DEVICE_PRIVATE)
 static inline swp_entry_t make_device_private_entry(struct page *page, bool write)
 {
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 75f322603d44..9bb44e1f968a 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -40,6 +40,9 @@
 unsigned int sysctl_sched_latency			= 6000000ULL;
 unsigned int normalized_sysctl_sched_latency		= 6000000ULL;
 
+unsigned int sysctl_sched_sync_hint_enable = 1;
+unsigned int sysctl_sched_cstate_aware = 1;
+
 /*
  * The initial- and re-scaling of tunables is configurable
  *
diff --git a/kernel/sysctl.c b/kernel/sysctl.c
index f8576509c7be..9095b4f060be 100644
--- a/kernel/sysctl.c
+++ b/kernel/sysctl.c
@@ -106,6 +106,7 @@ extern char core_pattern[];
 extern unsigned int core_pipe_limit;
 #endif
 extern int pid_max;
+extern int extra_free_kbytes;
 extern int pid_max_min, pid_max_max;
 extern int percpu_pagelist_fraction;
 extern int latencytop_enabled;
@@ -322,6 +323,13 @@ static struct ctl_table kern_table[] = {
 		.proc_handler	= proc_dointvec,
 	},
 #ifdef CONFIG_SCHED_DEBUG
+  {
+    .procname       = "sched_cstate_aware",
+    .data           = &sysctl_sched_cstate_aware,
+    .maxlen         = sizeof(unsigned int),
+    .mode           = 0644,
+    .proc_handler   = proc_dointvec,
+  },
 	{
 		.procname	= "sched_min_granularity_ns",
 		.data		= &sysctl_sched_min_granularity,
@@ -340,6 +348,13 @@ static struct ctl_table kern_table[] = {
 		.extra1		= &min_sched_granularity_ns,
 		.extra2		= &max_sched_granularity_ns,
 	},
+  {
+    .procname       = "sched_sync_hint_enable",
+    .data           = &sysctl_sched_sync_hint_enable,
+    .maxlen         = sizeof(unsigned int),
+    .mode           = 0644,
+    .proc_handler   = proc_dointvec,
+  },
 	{
 		.procname	= "sched_wakeup_granularity_ns",
 		.data		= &sysctl_sched_wakeup_granularity,
@@ -1461,6 +1476,14 @@ static struct ctl_table vm_table[] = {
 		.extra1		= &one,
 		.extra2		= &one_thousand,
 	},
+  {
+    .procname       = "extra_free_kbytes",
+    .data           = &extra_free_kbytes,
+    .maxlen         = sizeof(extra_free_kbytes),
+    .mode           = 0644,
+    .proc_handler   = min_free_kbytes_sysctl_handler,
+    .extra1         = &zero,
+  },
 	{
 		.procname	= "percpu_pagelist_fraction",
 		.data		= &percpu_pagelist_fraction,
@@ -1573,6 +1596,15 @@ static struct ctl_table vm_table[] = {
 		.mode		= 0644,
 		.proc_handler	= mmap_min_addr_handler,
 	},
+  {
+    .procname       = "mmap_noexec_taint",
+    .data           = &sysctl_mmap_noexec_taint,
+    .maxlen         = sizeof(sysctl_mmap_noexec_taint),
+    .mode           = 0644,
+    .proc_handler   = proc_dointvec_minmax,
+    .extra1         = &zero,
+    .extra2         = &one,
+  },
 #endif
 #ifdef CONFIG_NUMA
 	{
@@ -1644,6 +1676,13 @@ static struct ctl_table vm_table[] = {
 		.mode		= 0644,
 		.proc_handler	= proc_doulongvec_minmax,
 	},
+  {
+    .procname = "min_filelist_kbytes",
+    .data   = &min_filelist_kbytes,
+    .maxlen   = sizeof(min_filelist_kbytes),
+    .mode   = 0644,
+    .proc_handler = proc_dointvec,
+  },
 #ifdef CONFIG_HAVE_ARCH_MMAP_RND_BITS
 	{
 		.procname	= "mmap_rnd_bits",
@@ -1665,6 +1704,17 @@ static struct ctl_table vm_table[] = {
 		.extra1		= (void *)&mmap_rnd_compat_bits_min,
 		.extra2		= (void *)&mmap_rnd_compat_bits_max,
 	},
+#endif
+#ifdef CONFIG_DISK_BASED_SWAP
+  {
+   .procname       = "disk_based_swap",
+   .data           = &sysctl_disk_based_swap,
+   .maxlen         = sizeof(sysctl_disk_based_swap),
+   .mode           = 0644,
+   .proc_handler   = proc_dointvec_minmax,
+   .extra1         = &zero,
+   .extra2         = &one,
+  },
 #endif
 	{ }
 };
diff --git a/mm/Kconfig b/mm/Kconfig
index b457e94ae618..0712f9ce8c54 100644
--- a/mm/Kconfig
+++ b/mm/Kconfig
@@ -736,6 +736,17 @@ config DEVICE_PUBLIC
 	  memory; i.e., memory that is accessible from both the device and
 	  the CPU
 
+config LOW_MEM_NOTIFY
+  bool "Create device that lets processes detect low-memory conditions"
+  default n
+  help
+    A process can poll the /dev/low_mem device to be notified of
+    low-memory conditions.  The process can then attempt to free memory
+    before a OOM condition develops and the OOM killer takes over.  This
+    is meant to be used in systems with no or very little swap space.  In
+    the presence of large swap space, the system is likely to become
+    unusable before the OOM killer is triggered.
+
 config FRAME_VECTOR
 	bool
 
@@ -764,4 +775,13 @@ config GUP_BENCHMARK
 config ARCH_HAS_PTE_SPECIAL
 	bool
 
+config MM_METRICS
+   bool "Collect additional memory statistics"
+   help
+     Collect swap refault distances (seconds), swap read latencies and direct
+     reclaim latencies (nanoseconds). They are provided userspace via
+     /sys/kernel/debug/mm_metrics/{swap_refault,swap_latency,reclaim_latency}
+     in histograms.
+   default n
+
 endmenu
diff --git a/mm/Makefile b/mm/Makefile
index 26ef77a3883b..1370d6626857 100644
--- a/mm/Makefile
+++ b/mm/Makefile
@@ -100,8 +100,10 @@ obj-$(CONFIG_CMA_DEBUGFS) += cma_debug.o
 obj-$(CONFIG_USERFAULTFD) += userfaultfd.o
 obj-$(CONFIG_IDLE_PAGE_TRACKING) += page_idle.o
 obj-$(CONFIG_FRAME_VECTOR) += frame_vector.o
+obj-$(CONFIG_LOW_MEM_NOTIFY) += low-mem-notify.o
 obj-$(CONFIG_DEBUG_PAGE_REF) += debug_page_ref.o
 obj-$(CONFIG_HARDENED_USERCOPY) += usercopy.o
 obj-$(CONFIG_PERCPU_STATS) += percpu-stats.o
 obj-$(CONFIG_HMM) += hmm.o
 obj-$(CONFIG_MEMFD_CREATE) += memfd.o
+obj-$(CONFIG_MM_METRICS) += metrics.o
diff --git a/mm/low-mem-notify.c b/mm/low-mem-notify.c
new file mode 100644
index 000000000000..e89c34666446
--- /dev/null
+++ b/mm/low-mem-notify.c
@@ -0,0 +1,291 @@
+/*
+ * mm/low-mem-notify.c
+ *
+ * Sends low-memory notifications to processes via /dev/low-mem.
+ *
+ * Copyright (C) 2012 The Chromium OS Authors
+ * This program is free software, released under the GPL.
+ * Based on a proposal by Minchan Kim
+ *
+ * A process that polls /dev/low-mem is notified of a low-memory situation.
+ * The intent is to allow the process to free some memory before the OOM killer
+ * is invoked.
+ *
+ * A low-memory condition is estimated by subtracting anonymous memory
+ * (i.e. process data segments), kernel memory, and a fixed amount of
+ * file-backed memory from total memory.  This is just a heuristic, as in
+ * general we don't know how much memory can be reclaimed before we try to
+ * reclaim it, and that's too expensive or too late.
+ *
+ * This is tailored to Chromium OS, where a single program (the browser)
+ * controls most of the memory, and (currently) no swap space is used.
+ */
+
+
+#include <linux/low-mem-notify.h>
+#include <linux/module.h>
+#include <linux/sched.h>
+#include <linux/wait.h>
+#include <linux/poll.h>
+#include <linux/slab.h>
+#include <linux/mm.h>
+#include <linux/ctype.h>
+
+#define MB (1 << 20)
+
+static DECLARE_WAIT_QUEUE_HEAD(low_mem_wait);
+static atomic_t low_mem_state = ATOMIC_INIT(0);
+/* This is a list of thresholds in pages and should be in ascending order. */
+unsigned long low_mem_thresholds[LOW_MEM_THRESHOLD_MAX] = {
+	50 * MB / PAGE_SIZE };
+unsigned int low_mem_threshold_count = 1;
+
+/* last observed threshold */
+unsigned int low_mem_threshold_last = UINT_MAX;
+bool low_mem_margin_enabled = true;
+unsigned int low_mem_ram_vs_swap_weight = 4;
+
+/* Limit logging low memory to once per second. */
+DEFINE_RATELIMIT_STATE(low_mem_logging_ratelimit, 1 * HZ, 1);
+
+unsigned long low_mem_lowest_seen_anon_mem;
+const unsigned long low_mem_anon_mem_delta = 10 * MB / PAGE_SIZE;
+static struct kernfs_node *low_mem_available_dirent;
+
+struct low_mem_notify_file_info {
+	unsigned long unused;
+};
+
+void low_mem_notify(void)
+{
+	atomic_set(&low_mem_state, true);
+	wake_up(&low_mem_wait);
+}
+
+static int low_mem_notify_open(struct inode *inode, struct file *file)
+{
+	struct low_mem_notify_file_info *info;
+	int err = 0;
+
+	info = kmalloc(sizeof(*info), GFP_KERNEL);
+	if (!info) {
+		err = -ENOMEM;
+		goto out;
+	}
+
+	file->private_data = info;
+out:
+	return err;
+}
+
+static int low_mem_notify_release(struct inode *inode, struct file *file)
+{
+	kfree(file->private_data);
+	return 0;
+}
+
+static __poll_t low_mem_notify_poll(struct file *file, poll_table *wait)
+{
+	unsigned int ret = 0;
+
+	/* Update state to reflect any recent freeing. */
+	atomic_set(&low_mem_state, low_mem_check());
+
+	poll_wait(file, &low_mem_wait, wait);
+
+	if (low_mem_margin_enabled && atomic_read(&low_mem_state) != 0)
+		ret = POLLIN;
+
+	return ret;
+}
+
+const struct file_operations low_mem_notify_fops = {
+	.open = low_mem_notify_open,
+	.release = low_mem_notify_release,
+	.poll = low_mem_notify_poll,
+};
+EXPORT_SYMBOL(low_mem_notify_fops);
+
+#ifdef CONFIG_SYSFS
+
+#define LOW_MEM_ATTR(_name)				      \
+	static struct kobj_attribute low_mem_##_name##_attr = \
+		__ATTR(_name, 0644, low_mem_##_name##_show,   \
+		       low_mem_##_name##_store)
+
+static ssize_t low_mem_margin_show(struct kobject *kobj,
+				  struct kobj_attribute *attr, char *buf)
+{
+	if (low_mem_margin_enabled && low_mem_threshold_count) {
+		int i;
+		ssize_t written = 0;
+
+		for (i = 0; i < low_mem_threshold_count; i++)
+			written += sprintf(buf + written, "%lu ",
+			    low_mem_thresholds[i] * PAGE_SIZE / MB);
+		written += sprintf(buf + written, "\n");
+		return written;
+	} else
+		return sprintf(buf, "off\n");
+}
+
+static ssize_t low_mem_margin_store(struct kobject *kobj,
+				    struct kobj_attribute *attr,
+				    const char *buf, size_t count)
+{
+	int i = 0, consumed = 0;
+	const char *start = buf;
+	char *endp;
+	unsigned long thresholds[LOW_MEM_THRESHOLD_MAX];
+
+	memset(thresholds, 0, sizeof(thresholds));
+	/*
+	 * Even though the API does not say anything about this, the string in
+	 * buf is zero-terminated (as long as count < PAGE_SIZE) because buf is
+	 * a newly allocated zero-filled page.  Most other sysfs handlers rely
+	 * on this too.
+	 */
+	if (strncmp("off", buf, 3) == 0) {
+		pr_info("low_mem: disabling notifier\n");
+		low_mem_margin_enabled = false;
+		return count;
+	}
+	if (strncmp("on", buf, 2) == 0) {
+		pr_info("low_mem: enabling notifier\n");
+		low_mem_margin_enabled = true;
+		return count;
+	}
+	/*
+	 * This takes a space separated list of thresholds in ascending order,
+	 * and a trailing newline is optional.
+	 */
+	while (consumed < count) {
+		if (i >= LOW_MEM_THRESHOLD_MAX) {
+			pr_warn("low-mem: too many thresholds");
+			return -EINVAL;
+		}
+		/* special case for trailing newline */
+		if (*start == '\n')
+			break;
+
+		thresholds[i] = simple_strtoul(start, &endp, 0);
+		if ((endp == start) && *endp != '\n')
+			return -EINVAL;
+
+		/* make sure each is larger than the last one */
+		if (i && thresholds[i] <= thresholds[i - 1]) {
+			pr_warn("low-mem: thresholds not in increasing order: %lu then %lu\n",
+				thresholds[i - 1], thresholds[i]);
+			return -EINVAL;
+		}
+
+		if (thresholds[i] * (MB / PAGE_SIZE) > totalram_pages) {
+			pr_warn("low-mem: threshold too high\n");
+			return -EINVAL;
+		}
+
+		consumed += endp - start + 1;
+		start = endp + 1;
+		i++;
+	}
+
+	low_mem_threshold_count = i;
+	low_mem_margin_enabled = !!low_mem_threshold_count;
+
+	/* Convert to pages outside the allocator fast path. */
+	for (i = 0; i < low_mem_threshold_count; i++) {
+		low_mem_thresholds[i] =
+			thresholds[i] * (MB / PAGE_SIZE);
+		pr_info("low_mem: threshold[%d] %lu MB\n", i,
+			low_mem_thresholds[i] * PAGE_SIZE / MB);
+	}
+
+	return count;
+}
+LOW_MEM_ATTR(margin);
+
+static ssize_t low_mem_ram_vs_swap_weight_show(struct kobject *kobj,
+					       struct kobj_attribute *attr,
+					       char *buf)
+{
+	return sprintf(buf, "%u\n", low_mem_ram_vs_swap_weight);
+}
+
+static ssize_t low_mem_ram_vs_swap_weight_store(struct kobject *kobj,
+						struct kobj_attribute *attr,
+						const char *buf, size_t count)
+{
+	unsigned long weight;
+	int err;
+
+	err = kstrtoul(buf, 10, &weight);
+	if (err)
+		return -EINVAL;
+	/* The special value 0 represents infinity. */
+	low_mem_ram_vs_swap_weight = weight == 0 ?
+		-1U : (unsigned int) weight;
+	pr_info("low_mem: setting ram weight to %u\n",
+		low_mem_ram_vs_swap_weight);
+	return count;
+}
+LOW_MEM_ATTR(ram_vs_swap_weight);
+
+static ssize_t low_mem_available_show(struct kobject *kobj,
+				      struct kobj_attribute *attr,
+				      char *buf)
+{
+	unsigned long available_mem = get_available_mem_adj();
+
+	return sprintf(buf, "%lu\n",
+		       available_mem / (MB / PAGE_SIZE));
+}
+
+static ssize_t low_mem_available_store(struct kobject *kobj,
+				       struct kobj_attribute *attr,
+				       const char *buf, size_t count)
+{
+	return -EINVAL;
+}
+LOW_MEM_ATTR(available);
+
+static struct attribute *low_mem_attrs[] = {
+	&low_mem_margin_attr.attr,
+	&low_mem_ram_vs_swap_weight_attr.attr,
+	&low_mem_available_attr.attr,
+	NULL,
+};
+
+static struct attribute_group low_mem_attr_group = {
+	.attrs = low_mem_attrs,
+	.name = "chromeos-low_mem",
+};
+
+void low_mem_threshold_notify(void)
+{
+	if (low_mem_available_dirent)
+		sysfs_notify_dirent(low_mem_available_dirent);
+}
+
+static int __init low_mem_init(void)
+{
+	struct kernfs_node *low_mem_node;
+	int err = sysfs_create_group(mm_kobj, &low_mem_attr_group);
+	if (err)
+		pr_err("low_mem: register sysfs failed\n");
+
+	low_mem_node = sysfs_get_dirent(mm_kobj->sd, "chromeos-low_mem");
+	if (low_mem_node) {
+		low_mem_available_dirent =
+		    sysfs_get_dirent(low_mem_node, "available");
+		sysfs_put(low_mem_node);
+	}
+
+	if (!low_mem_available_dirent)
+		pr_warn("unable to find dirent for \"available\" attribute\n");
+
+	low_mem_lowest_seen_anon_mem = totalram_pages;
+	return err;
+}
+module_init(low_mem_init)
+
+#endif
diff --git a/mm/metrics.c b/mm/metrics.c
new file mode 100644
index 000000000000..81f3d9f41f43
--- /dev/null
+++ b/mm/metrics.c
@@ -0,0 +1,314 @@
+// SPDX-License-Identifier: GPL-2.0+
+
+#include <linux/ctype.h>
+#include <linux/mm.h>
+#include <linux/slab.h>
+#include <linux/swap.h>
+#include <linux/swapfile.h>
+#include <linux/debugfs.h>
+#include <linux/mm_metrics.h>
+
+/* make sure swapout timestamp won't wrap around within a year */
+#define SECONDS_PER_YEAR	(60 * 60 * 24 * 365)
+/* max number of buckets for histogram */
+#define MAX_HISTOGRAM_SIZE	100
+/* max number of digits in decimal for threshold plus one space */
+#define MAX_CHARS_PER_THRESHOLD	(20 + 1)
+
+bool swap_refault_enabled __read_mostly;
+struct histogram __rcu *mm_metrics_files[NR_MM_METRICS];
+
+static const char *const mm_metrics_names[] = {
+	"swap_refault",
+	"swap_latency",
+	"reclaim_latency",
+};
+
+static DEFINE_SPINLOCK(histogram_lock);
+
+static struct histogram *histogram_alloc(const u64 *thresholds,
+					 unsigned int size)
+{
+	int i;
+	int len;
+	struct histogram *hist;
+
+	VM_BUG_ON(!size || size > MAX_HISTOGRAM_SIZE);
+
+	len = sizeof(struct histogram) + size * sizeof(*hist->thresholds);
+	hist = kmalloc(len, GFP_ATOMIC);
+	if (!hist)
+		return ERR_PTR(-ENOMEM);
+
+	len = size * sizeof(*hist->buckets);
+	hist->buckets = __alloc_percpu_gfp(len, __alignof__(*hist->buckets),
+					   GFP_ATOMIC);
+	if (!hist->buckets) {
+		kfree(hist);
+		return ERR_PTR(-ENOMEM);
+	}
+
+	hist->size = size;
+	for (i = 0; i < size; i++) {
+		VM_BUG_ON(i && thresholds[i - 1] >= thresholds[i]);
+
+		hist->thresholds[i] = thresholds[i];
+	}
+	VM_BUG_ON(thresholds[i - 1] != U64_MAX);
+
+	return hist;
+}
+
+static struct histogram *histogram_create(char *buf)
+{
+	int i;
+	unsigned int size;
+	u64 *thresholds;
+	struct histogram *hist;
+
+	if (!*buf)
+		return ERR_PTR(-EINVAL);
+
+	thresholds = kmalloc_array(MAX_HISTOGRAM_SIZE, sizeof(*thresholds),
+				   GFP_KERNEL);
+	if (!thresholds)
+		return ERR_PTR(-ENOMEM);
+
+	for (i = 0; i < MAX_HISTOGRAM_SIZE; i++) {
+		thresholds[i] = simple_strtoull(buf, &buf, 0);
+		if (!*buf)
+			break;
+
+		if (!isspace(*buf)) {
+			hist = ERR_PTR(-EINVAL);
+			goto failed;
+		}
+
+		while (isspace(*buf))
+			buf++;
+	}
+
+	if (i == MAX_HISTOGRAM_SIZE) {
+		hist = ERR_PTR(-E2BIG);
+		goto failed;
+	}
+
+	/* the last theshold must be U64_MAX, add it if missing */
+	if (thresholds[i++] != U64_MAX) {
+		if (i == MAX_HISTOGRAM_SIZE) {
+			hist = ERR_PTR(-E2BIG);
+			goto failed;
+		}
+		thresholds[i++] = U64_MAX;
+	}
+
+	size = i;
+
+	for (i = 1; i < size; i++) {
+		if (thresholds[i - 1] >= thresholds[i]) {
+			hist = ERR_PTR(-EINVAL);
+			goto failed;
+		}
+	}
+
+	hist = histogram_alloc(thresholds, size);
+failed:
+	kfree(thresholds);
+
+	return hist;
+}
+
+static void histogram_free(struct rcu_head *rcu)
+{
+	struct histogram *hist = container_of(rcu, struct histogram, rcu);
+
+	VM_BUG_ON(!hist->size || hist->size > MAX_HISTOGRAM_SIZE);
+
+	free_percpu(hist->buckets);
+	kfree(hist);
+}
+
+static int mm_metrics_read(struct seq_file *sf, void *v)
+{
+	int i;
+	int cpu;
+	u64 *buckets;
+	struct histogram *hist;
+	int rc = 0;
+	unsigned int type = (unsigned long)sf->private;
+
+	VM_BUG_ON(type >= NR_MM_METRICS);
+
+	rcu_read_lock();
+
+	hist = rcu_dereference(mm_metrics_files[type]);
+	if (!hist) {
+		seq_puts(sf, "disabled\n");
+		goto unlock;
+	}
+
+	VM_BUG_ON(!hist->size || hist->size > MAX_HISTOGRAM_SIZE);
+
+	buckets = kmalloc_array(hist->size, sizeof(*buckets), GFP_NOWAIT);
+	if (!buckets) {
+		rc = -ENOMEM;
+		goto unlock;
+	}
+
+	memset(buckets, 0, hist->size * sizeof(*buckets));
+
+	for_each_possible_cpu(cpu) {
+		for (i = 0; i < hist->size; i++)
+			buckets[i] += per_cpu(hist->buckets[i], cpu);
+	}
+
+	for (i = 0; i < hist->size; i++) {
+		u64 lower = i ? hist->thresholds[i - 1] + 1 : 0;
+		u64 upper = hist->thresholds[i];
+
+		VM_BUG_ON(lower > upper);
+
+		seq_printf(sf, "%llu-%llu %llu\n", lower, upper, buckets[i]);
+	}
+	VM_BUG_ON(hist->thresholds[i - 1] != U64_MAX);
+
+	kfree(buckets);
+unlock:
+	rcu_read_unlock();
+
+	return rc;
+}
+
+static ssize_t mm_metrics_write(struct file *file, const char __user *buf,
+				size_t len, loff_t *ppos)
+{
+	char *raw, *trimmed;
+	struct histogram *old, *new = NULL;
+	unsigned int type = (unsigned long)file_inode(file)->i_private;
+
+	VM_BUG_ON(type >= NR_MM_METRICS);
+
+	if (len > MAX_HISTOGRAM_SIZE * MAX_CHARS_PER_THRESHOLD)
+		return -E2BIG;
+
+	raw = memdup_user_nul(buf, len);
+	if (IS_ERR(raw))
+		return PTR_ERR(raw);
+
+	trimmed = strim(raw);
+	if (!strcmp(trimmed, "clear")) {
+		rcu_read_lock();
+		old = rcu_dereference(mm_metrics_files[type]);
+		if (old)
+			new = histogram_alloc(old->thresholds, old->size);
+		rcu_read_unlock();
+	} else if (strcmp(trimmed, "disable"))
+		new = histogram_create(trimmed);
+
+	kfree(raw);
+
+	if (IS_ERR(new))
+		return PTR_ERR(new);
+
+	spin_lock(&histogram_lock);
+	old = rcu_dereference_protected(mm_metrics_files[type],
+					lockdep_is_held(&histogram_lock));
+	rcu_assign_pointer(mm_metrics_files[type], new);
+	spin_unlock(&histogram_lock);
+	if (old)
+		call_rcu(&old->rcu, histogram_free);
+
+	return len;
+}
+
+static int mm_metrics_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, mm_metrics_read, inode->i_private);
+}
+
+static const struct file_operations mm_metrics_ops = {
+	.open		= mm_metrics_open,
+	.write		= mm_metrics_write,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+};
+
+static int __init mm_metrics_init(void)
+{
+	int i;
+	struct dentry *dent;
+#ifdef CONFIG_SWAP
+	unsigned long now = ktime_get_seconds();
+	unsigned long size = max_swapfile_size();
+
+	if (SWP_TM_OFF_BITS > FIELD_SIZEOF(swp_entry_t, val) * BITS_PER_BYTE)
+		pr_err("swap refault metrics disabled: 32-bit CPU\n");
+	else if (size < GENMASK_ULL(SWP_TM_OFF_BITS - 1, 0) + 1)
+		pr_err("swap refault metrics disabled: size %ld\n", size);
+	else if (now + SECONDS_PER_YEAR > BIT_ULL(SWP_TIME_BITS))
+		pr_err("swap refault metrics disabled: time %ld\n", now);
+	else
+		swap_refault_enabled = true;
+#endif
+
+	BUILD_BUG_ON(ARRAY_SIZE(mm_metrics_names) != NR_MM_METRICS);
+
+	if (!debugfs_initialized())
+		return -ENODEV;
+
+	dent = debugfs_create_dir("mm_metrics", NULL);
+	if (!dent)
+		return -ENODEV;
+
+	for (i = 0; i < NR_MM_METRICS; i++) {
+		struct dentry *fent;
+
+		if (i == MM_SWAP_REFAULT && !swap_refault_enabled)
+			continue;
+
+		fent = debugfs_create_file(mm_metrics_names[i], 0644, dent,
+					   (void *)(long)i, &mm_metrics_ops);
+		if (IS_ERR_OR_NULL(fent)) {
+			debugfs_remove_recursive(dent);
+
+			return -ENODEV;
+		}
+	}
+
+	pr_info("memory metrics initialized\n");
+
+	return 0;
+}
+subsys_initcall(mm_metrics_init);
+
+void mm_metrics_record(unsigned int type, u64 val, u64 count)
+{
+	int lower, upper;
+	struct histogram *hist;
+
+	VM_BUG_ON(type >= NR_MM_METRICS);
+
+	rcu_read_lock();
+
+	hist = rcu_dereference(mm_metrics_files[type]);
+	if (!hist)
+		goto unlock;
+
+	VM_BUG_ON(!hist->size || hist->size > MAX_HISTOGRAM_SIZE);
+
+	lower = 0;
+	upper = hist->size - 1;
+	while (lower < upper) {
+		int i = (lower + upper) >> 1;
+
+		if (val <= hist->thresholds[i])
+			upper = i;
+		else
+			lower = i + 1;
+	}
+
+	this_cpu_add(hist->buckets[upper], count);
+unlock:
+	rcu_read_unlock();
+}
diff --git a/mm/mmap.c b/mm/mmap.c
index 1480880ff814..9f9b1e6b8372 100644
--- a/mm/mmap.c
+++ b/mm/mmap.c
@@ -1485,7 +1485,8 @@ unsigned long do_mmap(struct file *file, unsigned long addr,
 			if (path_noexec(&file->f_path)) {
 				if (vm_flags & VM_EXEC)
 					return -EPERM;
-				vm_flags &= ~VM_MAYEXEC;
+        if (sysctl_mmap_noexec_taint)
+				  vm_flags &= ~VM_MAYEXEC;
 			}
 
 			if (!file->f_op->mmap)
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 57e63daf821f..00f86d08f989 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -66,6 +66,8 @@
 #include <linux/ftrace.h>
 #include <linux/lockdep.h>
 #include <linux/nmi.h>
+#include <linux/low-mem-notify.h>
+#include <linux/mm_metrics.h>
 
 #include <asm/sections.h>
 #include <asm/tlbflush.h>
@@ -265,6 +267,14 @@ int min_free_kbytes = 1024;
 int user_min_free_kbytes = -1;
 int watermark_scale_factor = 10;
 
+/*
+ * Extra memory for the system to try freeing. Used to temporarily
+ * free memory, to make space for new workloads. Anyone can allocate
+ * down to the min watermarks controlled by min_free_kbytes above.
+ */
+int extra_free_kbytes = 0;
+
+
 static unsigned long nr_kernel_pages __meminitdata;
 static unsigned long nr_all_pages __meminitdata;
 static unsigned long dma_reserve __meminitdata;
@@ -4368,6 +4378,7 @@ __alloc_pages_nodemask(gfp_t gfp_mask, unsigned int order, int preferred_nid,
 	unsigned int alloc_flags = ALLOC_WMARK_LOW;
 	gfp_t alloc_mask; /* The gfp_t that was actually used for allocation */
 	struct alloc_context ac = { };
+  u64 start = 0;
 
 	/*
 	 * There are several places where we assume that the order value is sane
@@ -4385,6 +4396,8 @@ __alloc_pages_nodemask(gfp_t gfp_mask, unsigned int order, int preferred_nid,
 
 	finalise_ac(gfp_mask, &ac);
 
+  low_mem_check();
+
 	/* First allocation attempt */
 	page = get_page_from_freelist(alloc_mask, order, alloc_flags, &ac);
 	if (likely(page))
@@ -4406,8 +4419,11 @@ __alloc_pages_nodemask(gfp_t gfp_mask, unsigned int order, int preferred_nid,
 	if (unlikely(ac.nodemask != nodemask))
 		ac.nodemask = nodemask;
 
+  if (order < MAX_ORDER && (gfp_mask & __GFP_DIRECT_RECLAIM) &&
+      !(current->flags & PF_MEMALLOC))
+          start = mm_metrics_reclaim_start();
 	page = __alloc_pages_slowpath(alloc_mask, order, &ac);
-
+  mm_metrics_reclaim_end(start);
 out:
 	if (memcg_kmem_enabled() && (gfp_mask & __GFP_ACCOUNT) && page &&
 	    unlikely(memcg_kmem_charge(page, gfp_mask, order) != 0)) {
@@ -7237,6 +7253,7 @@ static void setup_per_zone_lowmem_reserve(void)
 static void __setup_per_zone_wmarks(void)
 {
 	unsigned long pages_min = min_free_kbytes >> (PAGE_SHIFT - 10);
+  unsigned long pages_low = extra_free_kbytes >> (PAGE_SHIFT - 10);
 	unsigned long lowmem_pages = 0;
 	struct zone *zone;
 	unsigned long flags;
@@ -7248,11 +7265,14 @@ static void __setup_per_zone_wmarks(void)
 	}
 
 	for_each_zone(zone) {
-		u64 tmp;
+		u64 min, low;
 
 		spin_lock_irqsave(&zone->lock, flags);
-		tmp = (u64)pages_min * zone->managed_pages;
-		do_div(tmp, lowmem_pages);
+		min = (u64)pages_min * zone->managed_pages;
+		do_div(min, lowmem_pages);
+    low = (u64)pages_low * zone->managed_pages;
+    do_div(low, vm_total_pages);
+
 		if (is_highmem(zone)) {
 			/*
 			 * __GFP_HIGH and PF_MEMALLOC allocations usually don't
@@ -7273,7 +7293,7 @@ static void __setup_per_zone_wmarks(void)
 			 * If it's a lowmem zone, reserve a number of pages
 			 * proportionate to the zone's size.
 			 */
-			zone->watermark[WMARK_MIN] = tmp;
+			zone->watermark[WMARK_MIN] = min;
 		}
 
 		/*
@@ -7281,12 +7301,12 @@ static void __setup_per_zone_wmarks(void)
 		 * scale factor in proportion to available memory, but
 		 * ensure a minimum size on small systems.
 		 */
-		tmp = max_t(u64, tmp >> 2,
+		min = max_t(u64, min >> 2,
 			    mult_frac(zone->managed_pages,
 				      watermark_scale_factor, 10000));
 
-		zone->watermark[WMARK_LOW]  = min_wmark_pages(zone) + tmp;
-		zone->watermark[WMARK_HIGH] = min_wmark_pages(zone) + tmp * 2;
+		zone->watermark[WMARK_LOW]  = min_wmark_pages(zone) + low + min;
+		zone->watermark[WMARK_HIGH] = min_wmark_pages(zone) + low + min * 2;
 
 		spin_unlock_irqrestore(&zone->lock, flags);
 	}
@@ -7369,7 +7389,7 @@ core_initcall(init_per_zone_wmark_min)
 /*
  * min_free_kbytes_sysctl_handler - just a wrapper around proc_dointvec() so
  *	that we can call two helper functions whenever min_free_kbytes
- *	changes.
+ *	or extra_free_kbytes changes.
  */
 int min_free_kbytes_sysctl_handler(struct ctl_table *table, int write,
 	void __user *buffer, size_t *length, loff_t *ppos)
@@ -7972,6 +7992,8 @@ int alloc_contig_range(unsigned long start, unsigned long end,
 
 	/* Make sure the range is really isolated. */
 	if (test_pages_isolated(outer_start, end, false)) {
+    pr_info_ratelimited("%s: [%lx, %lx) PFNs busy\n",
+      __func__, outer_start, end);
 		ret = -EBUSY;
 		goto done;
 	}
diff --git a/mm/util.c b/mm/util.c
index 6a24a1025d77..0dee90b8729f 100644
--- a/mm/util.c
+++ b/mm/util.c
@@ -563,6 +563,7 @@ int sysctl_overcommit_memory __read_mostly = OVERCOMMIT_GUESS;
 int sysctl_overcommit_ratio __read_mostly = 50;
 unsigned long sysctl_overcommit_kbytes __read_mostly;
 int sysctl_max_map_count __read_mostly = DEFAULT_MAX_MAP_COUNT;
+int sysctl_mmap_noexec_taint __read_mostly = 0;
 unsigned long sysctl_user_reserve_kbytes __read_mostly = 1UL << 17; /* 128MB */
 unsigned long sysctl_admin_reserve_kbytes __read_mostly = 1UL << 13; /* 8MB */
 
diff --git a/mm/vmscan.c b/mm/vmscan.c
index b37610c0eac6..4a17b57b7cfb 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -166,6 +166,11 @@ int vm_swappiness = 60;
  */
 unsigned long vm_total_pages;
 
+/*
+ * Low watermark used to prevent fscache thrashing during low memory.
+ */
+int min_filelist_kbytes;
+
 static LIST_HEAD(shrinker_list);
 static DECLARE_RWSEM(shrinker_rwsem);
 
@@ -2242,6 +2247,25 @@ static bool inactive_list_is_low(struct lruvec *lruvec, bool file,
 	return inactive * inactive_ratio < active;
 }
 
+/*
+ * Check low watermark used to prevent fscache thrashing during low memory.
+ */
+static int file_is_low(struct lruvec *lruvec, struct scan_control *sc)
+{
+  unsigned long pages_min, active, inactive;
+  enum lru_list inactive_lru = LRU_FILE;
+  enum lru_list active_lru = LRU_FILE + LRU_ACTIVE;
+
+  if (!mem_cgroup_disabled())
+    return false;
+
+  pages_min = min_filelist_kbytes >> (PAGE_SHIFT - 10);
+  inactive = lruvec_lru_size(lruvec, inactive_lru, sc->reclaim_idx);
+  active = lruvec_lru_size(lruvec, active_lru, sc->reclaim_idx);
+
+  return ((active + inactive) < pages_min);
+}
+
 static unsigned long shrink_list(enum lru_list lru, unsigned long nr_to_scan,
 				 struct lruvec *lruvec, struct scan_control *sc)
 {
@@ -2285,6 +2309,15 @@ static void get_scan_count(struct lruvec *lruvec, struct mem_cgroup *memcg,
 	unsigned long ap, fp;
 	enum lru_list lru;
 
+  /*
+   * Do not scan file pages when swap is allowed by __GFP_IO and
+   * file page count is low.
+   */
+  if ((sc->gfp_mask & __GFP_IO) && file_is_low(lruvec, sc)) {
+    scan_balance = SCAN_ANON;
+    goto out;
+  }
+
 	/* If we have no swap space, do not bother scanning anon pages. */
 	if (!sc->may_swap || mem_cgroup_get_nr_swap_pages(memcg) <= 0) {
 		scan_balance = SCAN_FILE;
